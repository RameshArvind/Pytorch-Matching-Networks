{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torch Matching Net - Omniglot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "nsnJVyFjvwDB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# One Shot learning on Omniglot Dataset\n",
        "\n",
        "Our goal as part of this notebook is to solve the [one-shot learning problem](https://en.wikipedia.org/wiki/One-shot_learning) for the [omniglot dataset](https://github.com/brendenlake/omniglot). We aim to learn a model that given a set of images, its associated labels and a target image, we should be able to correctly assign the label of the target image from the provided labels.\n",
        "##Example of a one shot classification task\n",
        "![One shot Task](https://imgur.com/tEsYNaZ "One Shot Task")\n",
        "\n",
        "Here our goal would be to match the 2nd Image of the first row to the single image shown in last row (as indicated by the labels)"
      ]
    },
    {
      "metadata": {
        "id": "sqluy6IhG9wh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Installing Requirements and Imports\n",
        "\n",
        "We will be using **PyTorch** for defining and training our model and **comet.ml** for logging our stats"
      ]
    },
    {
      "metadata": {
        "id": "ot-iNuia-VUt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision\n",
        "!pip3 install comet_ml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fY3hj2pE-HAP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from comet_ml import Experiment\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3m4zN9QeHFo5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Model Architecture"
      ]
    },
    {
      "metadata": {
        "id": "j6stLGjEavMT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convolutional Block\n",
        "A Standard convolutional block which follows: <br>\n",
        "Image --> Convolutional layer --> ReLU Activation --> Batch Normalization --> Pooling Layer --> Dropout.<br>\n",
        "The dropout layer has been added for improved regularization."
      ]
    },
    {
      "metadata": {
        "id": "ZuPjY1Eg8roX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvLayerWithBatchNorm(nn.Module):\n",
        "  \n",
        "  def __init__(self, in_channels, out_channels=64, kernel_size=3, padding=1, dropout_probality=0.2):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding)\n",
        "    self.ReLU = nn.ReLU()\n",
        "    self.batch_norm_layer = nn.BatchNorm2d(out_channels)\n",
        "    self.maxpool = nn.MaxPool2d(2, 2)\n",
        "    self.dropout = nn.Dropout(dropout_probality) # Dropout to add regularization and improve model generalization\n",
        "    \n",
        "  def forward(self, X):\n",
        "    x = self.conv(X)\n",
        "    x = self.ReLU(x)\n",
        "    x = self.batch_norm_layer(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.dropout(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0XdMt0F4f6oA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Embedding Layer\n",
        "This is the embedding model, which is a stack of 4 of the above `ConvLayerWithBatchNorm` layers. <br>\n",
        "An addtional fully connected layer is added at the end along with dropout"
      ]
    },
    {
      "metadata": {
        "id": "tP0nQ38G-SBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvEmbedding(nn.Module):\n",
        "  \n",
        "  def __init__(self, in_channels=1, embedding_size=256, dropout_probality=0.2):\n",
        "    super().__init__()\n",
        "    self.conv1 = ConvLayerWithBatchNorm(in_channels, 64, dropout_probality=dropout_probality)\n",
        "    self.conv2 = ConvLayerWithBatchNorm(64, 64, dropout_probality=dropout_probality)\n",
        "    self.conv3 = ConvLayerWithBatchNorm(64, 64, dropout_probality=dropout_probality)\n",
        "    self.conv4 = ConvLayerWithBatchNorm(64, 64, dropout_probality=dropout_probality)\n",
        "    self.dense = nn.Linear(64, embedding_size)\n",
        "    self.dropout = nn.Dropout(dropout_probality) # Dropout to add regularization and improve model generalization\n",
        "    self.embedding_size = embedding_size\n",
        "  \n",
        "  def forward(self, X):\n",
        "    # Input shape is (batch_size, 1, 28, 28)\n",
        "    x = self.conv1(X) # x's Shape (batch_size, 64, 14, 14)\n",
        "    x = self.conv2(x) # x's Shape (batch_size, 64, 7, 7)\n",
        "    x = self.conv3(x) # x's Shape (batch_size, 64, 3, 3)\n",
        "    x = self.conv4(x) # x's Shape (batch_size, 64, 1, 1)\n",
        "    x = x.view(x.size()[0], -1) # x's Shape (batch_size, 64)\n",
        "    x = self.dense(x) # x's Shape (batch_size, embedding_size)\n",
        "    x = self.dropout(x) # x's Shape (batch_size, embedding_size) with a few activations flipped to 0\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cejO4ZTSnb7p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  The Fully Conditional Embedding - Target Image\n",
        "Now we have the fully conditional embedding layer for our target image. This allows us to obtain an embedding for our target image in the context of the support images.\n",
        "\n",
        "One significant change in our implementation is that instead of concatenating the hidden state and our attended output, we choose to add them instead. This is due to to the available LSTMCell has hidden weights of shape (hidden_size, hidden_size) but in order to take the concatenated input as the hidden state, we must construct our custom LSTMCell which has hidden weights of size (2xhidden_size, hidden_size).\n",
        "\n",
        "This can be found mentioned by one of the authors Oriol Vinayls [on machine learning subreddit here](https://www.reddit.com/r/MachineLearning/comments/6efl5g/d_order_matters_attention_mechanisms/)"
      ]
    },
    {
      "metadata": {
        "id": "RSInVD2Gtk--",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullyConditionalEmbeddingTargetImage(nn.Module):\n",
        "  \n",
        "  def __init__(self, embedding_size, processing_steps=10):\n",
        "    super().__init__()\n",
        "    self.lstm_cell = torch.nn.LSTMCell(embedding_size, embedding_size)\n",
        "    self.processing_steps = processing_steps\n",
        "    self.embedding_size = embedding_size\n",
        "    self.attn_softmax = nn.Softmax(dim=1)\n",
        "  def forward(self, target_image_encoded, support_images_encoded):\n",
        "    batch_size, num_images, _ = support_images_encoded.shape\n",
        "    \n",
        "#     hidden_state_prev = torch.zeros(batch_size, self.embedding_size).to(device)\n",
        "    cell_state_prev = torch.zeros(batch_size, self.embedding_size).to(device)\n",
        "    hidden_state_prev = torch.sum(support_images_encoded, dim=1) / num_images\n",
        "    for i in range(self.processing_steps):\n",
        "      hidden_out, cell_out = self.lstm_cell(target_image_encoded, (hidden_state_prev, cell_state_prev))\n",
        "      hidden_out = hidden_out + target_image_encoded\n",
        "      attn = self.attn_softmax(torch.bmm(support_images_encoded, hidden_out.unsqueeze(2)))\n",
        "      attended_values = torch.sum(attn * support_images_encoded, dim=1)\n",
        "      hidden_state_prev = hidden_out + attended_values\n",
        "      cell_state_prev = cell_out\n",
        "    \n",
        "    return hidden_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vbyJSn3j5H0A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Fully Conditional Embedding - Support Images\n",
        "\n",
        "Here we have the the fully conditional embeddings for our support images, this is similar to our previous layer for the target image except for the difference that it is just a simple bi-directional LSTM where we just pass our support images sequence.\n",
        "\n",
        "Our new embeddings are going to be the LSTM's **Forward Activation + Backward Activation + Support Image Embedding**"
      ]
    },
    {
      "metadata": {
        "id": "AEmsFcZMr0Hq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullyConditionalEmbeddingSupportImages(nn.Module):\n",
        "  \n",
        "  def __init__(self, embedding_size):\n",
        "    super().__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "    self.bidirectionalLSTM = nn.LSTM(input_size=embedding_size, hidden_size=embedding_size, bidirectional=True, batch_first=True)\n",
        "  \n",
        "  def initialize_hidden(self, batch_size):\n",
        "    #Initialize the states needed for our bi-directional LSTM\n",
        "    hidden_state = torch.zeros(2, batch_size, self.embedding_size).to(device)\n",
        "    cell_state = torch.zeros(2, batch_size, self.embedding_size).to(device)\n",
        "    return (hidden_state, cell_state)\n",
        "  \n",
        "  def forward(self, support_embeddings):\n",
        "    batch_size, num_images, _ = support_embeddings.shape\n",
        "    # Initialize states\n",
        "    lstm_states = self.initialize_hidden(batch_size)\n",
        "    # Get the LSTM Outputs\n",
        "    support_embeddings_contextual, internal_states = self.bidirectionalLSTM(support_embeddings, lstm_states)\n",
        "    # Get the forward and backward outputs\n",
        "    support_embeddings_contextual = support_embeddings_contextual.view(batch_size, num_images, 2, self.embedding_size)\n",
        "    # Add the forward and backward outputs\n",
        "    support_embeddings_contextual = torch.sum(support_embeddings_contextual, dim=2)\n",
        "    # Add the skip connection to our output\n",
        "    support_embeddings_contextual = support_embeddings_contextual + support_embeddings\n",
        "    return support_embeddings_contextual"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o82Te4-sgQ-C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Cosine Distance Module\n",
        "A cosine distance module that allows us to compute the cosine distance of our target image with each of the support images"
      ]
    },
    {
      "metadata": {
        "id": "MO31uRMo-QtZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CosineDistance(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  \n",
        "  def forward(self, target_image, support_images):\n",
        "    support_images_normed = F.normalize(support_images, p=2, dim=2)\n",
        "    # the 'p=2' param represents squared norm\n",
        "    target_image_normed = F.normalize(target_image, p=2, dim=1)\n",
        "    target_image_normed = target_image_normed.unsqueeze(dim=1).permute(0, 2, 1)\n",
        "    # This will cause the dimensions to be [5, 64, 1]\n",
        "    similarities = torch.bmm(support_images_normed, target_image.unsqueeze(1).permute(0, 2, 1))\n",
        "    # torch.bmm = batch matrix multiply\n",
        "    # [5, 20, 64] @ [5, 64, 1]\n",
        "    # the output shape is [5, 20, 1]\n",
        "    similarities = similarities.squeeze(dim=2)\n",
        "    # remove last dimension\n",
        "    return similarities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K2REH_O-gaGk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##One-Hot Converter Module\n",
        "A helper class that allows us to convert labels into one-hot vectors"
      ]
    },
    {
      "metadata": {
        "id": "SrBPpY-nyjFh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Taken from @activatedgeeks's answer from https://stackoverflow.com/questions/44461772/creating-one-hot-vector-from-indices-given-as-a-tensor\n",
        "class ConvertOneHot(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  \n",
        "  def forward(self, labels, num_classes):\n",
        "    batch_size, num_images, _ = labels.size()\n",
        "    one_hot_labels = torch.Tensor(batch_size, num_images, num_classes).to(labels.device).float().zero_()\n",
        "    return one_hot_labels.scatter(2, labels, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RKHpVs7CggJU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Matching Network\n",
        "The orchestrating module that builds the entire matching network.<br> We implementing the matching networks described in this [paper](https://arxiv.org/pdf/1606.04080.pdf)<br>\n",
        "\n",
        "For an overview of the paper read up on this blogpost from [Adrian Colyer on it](https://blog.acolyer.org/2017/01/03/matching-networks-for-one-shot-learning/) \n",
        "\n",
        "**Input to the Network**: *Support Images*, *Support Labels* and *Target Image*<br>\n",
        "**Output of the Network**: *One-Hot encoded Target label*<br>\n",
        "\n",
        "1. The model operates by first getting the embeddings for both support images and our target image via the `ConvEmbedding` model.\n",
        "* Then we calculate the consine distance between the target image embedding and each of the support image embedding.\n",
        "* We squash the distances via a softmax function. This acts as our attention weights\n",
        "* We multiply the attention weights with our one-hot encoded labels and sum them in order to get our predicted one-hot label.\n",
        "* We then measure the cross entropy loss of our predicted label and the expected target label and backpropagate.\n",
        "\n",
        "Drew Heavy inspiration from the following Matching Networks implementations:\n",
        "\n",
        "\n",
        "1.   BoyuanJiang's implementation - https://github.com/BoyuanJiang/matching-networks-pytorch\n",
        "2.   activatedgeek's implementation - https://github.com/activatedgeek/Matching-Networks\n",
        "3.   Mark's implementation for the Full Context Embeddings - https://github.com/markdtw/matching-networks\n"
      ]
    },
    {
      "metadata": {
        "id": "vKzh31O3-f6I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MatchingNet(nn.Module):\n",
        "  \n",
        "  def __init__(self, image_shape, embedding_size=256, dropout_probality=0.2, use_fce=True):\n",
        "    super().__init__()\n",
        "    self.attn = nn.Softmax(dim=1)\n",
        "    self.embedding = ConvEmbedding(embedding_size=embedding_size, dropout_probality=dropout_probality)\n",
        "    self.distance = CosineDistance()\n",
        "    self.use_fce = use_fce\n",
        "    self.onehotconverter = ConvertOneHot()\n",
        "    if self.use_fce:\n",
        "      self.full_conditional_embedding_support = FullyConditionalEmbeddingSupportImages(embedding_size=embedding_size)\n",
        "      self.full_conditional_embedding_target = FullyConditionalEmbeddingTargetImage(embedding_size=embedding_size)\n",
        "    self.image_shape = image_shape\n",
        "  \n",
        "  def forward(self, support_images, support_labels, target_image):\n",
        "\n",
        "    batch_size, num_images, _ = support_labels.size()\n",
        "\n",
        "    # Get the image encodings from convolutional embedding\n",
        "    target_image_encoded = self.embedding(target_image)\n",
        "    support_images_encoded = self.embedding(support_images.view(-1, *self.image_shape)).view(-1, num_images, self.embedding.embedding_size)\n",
        "    \n",
        "    if self.use_fce:\n",
        "      # Get the support images embedding with context\n",
        "      support_images_encoded = self.full_conditional_embedding_support(support_images_encoded)\n",
        "\n",
        "      # Get the target image embedding with context\n",
        "      target_image_encoded = self.full_conditional_embedding_target(target_image_encoded, support_images_encoded)\n",
        "    \n",
        "    # Get the cosine distances between target image and the support images\n",
        "    distances = self.distance(target_image_encoded, support_images_encoded)\n",
        "    \n",
        "    # Get the attention value based on the distances\n",
        "    attention = self.attn(distances)\n",
        "    \n",
        "    # Convert the labels into one hot vectors\n",
        "    support_set_one_hot_labels = self.onehotconverter(support_labels, num_images)\n",
        "    \n",
        "    # Get the prediction logits by attention * one-hot-labels (automatically summed due to the unsqueeze operation)\n",
        "    prediction_logits = torch.bmm(attention.unsqueeze(1), support_set_one_hot_labels).squeeze()\n",
        "    \n",
        "    # Get the final labels for predictions\n",
        "    _, prediction_labels = torch.max(prediction_logits, 1)\n",
        "    return prediction_logits, prediction_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XNtPR5jgHKtY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Testing the model \n",
        "\n",
        "We just run the model via random data over a few epochs to ensure that it is able to learn something and ensure that the gradients propagate correctly i.e catch any silly mistakes like dimension mismatch etc"
      ]
    },
    {
      "metadata": {
        "id": "GaYlaW7KG4Zt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_shape = (1, 28, 28)\n",
        "matching_net_trial = MatchingNet(img_shape, dropout_probality=0.1, use_fce=False)\n",
        "print(\"Model Summary\")\n",
        "print(matching_net_trial)\n",
        "epochs = 10\n",
        "\n",
        "support_images = torch.rand(32, 20, *img_shape)\n",
        "target_image = torch.rand(32, *img_shape)\n",
        "support_labels = torch.LongTensor(32, 20, 1) % 20\n",
        "target_labels = torch.LongTensor(32) % 20\n",
        "\n",
        "matching_net_trial.to(device)\n",
        "support_images = support_images.to(device)\n",
        "support_labels = support_labels.to(device)\n",
        "target_image = target_image.to(device)\n",
        "target_labels = target_labels.to(device)\n",
        "optimizer = torch.optim.Adam(matching_net_trial.parameters(), lr=0.001)\n",
        "for epoch in range(epochs):\n",
        "  logits, predictions = matching_net_trial(support_images, support_labels,target_image)\n",
        "  loss = F.cross_entropy(logits, target_labels)\n",
        "  print(loss.item())\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PfgmRLj0HSG3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Gathering the Data\n",
        "\n",
        "We fetch the data from github repository and unzip the train and evaluation zip files"
      ]
    },
    {
      "metadata": {
        "id": "cLtpEjzOIU-3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/brendenlake/omniglot.git\n",
        "!unzip -qq omniglot/python/images_background.zip\n",
        "!unzip -qq omniglot/python/images_evaluation.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cLMlgkkuKtvE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Helper functions to read and rotate images"
      ]
    },
    {
      "metadata": {
        "id": "PdbvPNTh0n6Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from skimage import io, transform\n",
        "import numpy as np\n",
        "\n",
        "def read_image(path, size, angle=0):\n",
        "  img = io.imread(path, as_grey=True)\n",
        "  img = transform.resize(img, size, mode='constant')\n",
        "#   img = 1 - np.expand_dims(img, 0)\n",
        "  img = np.expand_dims(img, 0)\n",
        "  return img\n",
        "\n",
        "def rotate(img, angle):\n",
        "  return np.expand_dims(transform.rotate(np.squeeze(img), angle), 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qBwYPG4K_oxZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "PATH = 'images_background'\n",
        "EVALUATION_PATH = 'images_evaluation'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-8Z3b3H2K04Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We recursively go over each Language and Alphabet and extract the images of each Letter present in them into a dictionary.<br>\n",
        "We also extract the images here into numpy arrays for both the \"Train\" and \"Evaluation\" sets."
      ]
    },
    {
      "metadata": {
        "id": "7DL1jQoO_G0-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_alphabet(path):\n",
        "  lang_dict = []\n",
        "  for alphabet in os.listdir(path):\n",
        "    alphabet_path = os.path.join(path, alphabet)\n",
        "    for letter in os.listdir(alphabet_path):\n",
        "      current_letter_dict = {\"alphabet\": alphabet, \"letter\":letter, \"images\":[], \"image_mat\": []}\n",
        "      letter_path = os.path.join(alphabet_path, letter)\n",
        "      for letter_image in os.listdir(letter_path):\n",
        "        letter_image_path = os.path.join(letter_path, letter_image)\n",
        "        current_letter_dict[\"images\"].append(letter_image_path)\n",
        "        img = read_image(letter_image_path, (28, 28))\n",
        "        current_letter_dict[\"image_mat\"].append(img)\n",
        "      lang_dict.append(current_letter_dict)\n",
        "  return lang_dict\n",
        "\n",
        "train_dict = load_alphabet(PATH)\n",
        "test_dict = load_alphabet(EVALUATION_PATH)\n",
        "print(f\"found letters in Training set - {len(train_dict)}\")\n",
        "print(f\"found letters in Testing set - {len(test_dict)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z-Y4rWPGLSJJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We also define an augmentation function that addtionally generates 4 rotations of each alphabet thus increasing the number of classes we possess by a factor of 4. There is still some issue with the model when it utilizes augmented data so for now we will not be using it."
      ]
    },
    {
      "metadata": {
        "id": "JqwrW6OdcDWX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def augment_dataset(dataset):\n",
        "  augmented_dataset = []\n",
        "  for letter in dataset:\n",
        "    for angle in range(4):\n",
        "      current_letter_dict = {\"alphabet\": letter[\"alphabet\"], \n",
        "                             \"letter\":letter[\"letter\"], \n",
        "                             \"images\":letter[\"images\"], \n",
        "                             \"image_mat\": []}\n",
        "      for image in letter[\"image_mat\"]:\n",
        "        rotated_image = rotate(image, angle*90)\n",
        "        current_letter_dict[\"image_mat\"].append(rotated_image)\n",
        "      augmented_dataset.append(current_letter_dict)\n",
        "  return augmented_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mUKSdNAIHWHS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Splitting data into Train/Dev/Test Sets\n",
        "Using sklearn's `train_test_split` function we create 3 sets of data.\n",
        "\n",
        "\n",
        "1.   **Training Set** - Used to train our model\n",
        "2.   **Dev Set** - Used to evaluate our model during the training process in order to perform hyperparameter tuning\n",
        "3.   **Testing Set** - Used to finally evaluate our model\n",
        "\n",
        "Before generating the above three sets, we first combine the provided Train and Test sets since they seem to be coming from different distributions and the algorithm was performing poorly when using the provided Train/Test Split.\n",
        "\n",
        "We set aside 25% of total data for testing set.\n",
        "\n",
        "From the remaining 75% of data we again split the data in 9:1 ratio. 67.5% of total data for training set and 7.5% of total data for dev set\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Um5ZxBm0G-IM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "random_state = 50\n",
        "total_data = train_dict + test_dict\n",
        "training_dict, testing_dict = train_test_split(total_data, test_size=0.25, random_state=random_state)\n",
        "training_dict, dev_dict = train_test_split(training_dict, test_size=0.1, random_state=random_state)\n",
        "\n",
        "print(f\"Total Dataset size - {len(total_data)}\")\n",
        "print(f\"Training Dataset size - {len(training_dict)}\")\n",
        "print(f\"Dev Dataset size - {len(dev_dict)}\")\n",
        "print(f\"Testing Dataset size - {len(testing_dict)}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bl74JkPedQQL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# augmented_train_dict = augment_dataset(training_dict)\n",
        "# augmented_dev_dict = augment_dataset(dev_dict)\n",
        "augmented_train_dict = training_dict\n",
        "augmented_dev_dict = dev_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "83ghW7TYHgbX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Creating the tasks for our Model"
      ]
    },
    {
      "metadata": {
        "id": "XKBmrVe2NNFI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We define a function that takes one of the datasets and creates a ***N-Way*** one shot task where we choose ***N*** different letters (i.e Unique Characters from our dataset) and take one random image from each of them. Additionally one extra Image is chosen from one of the ***N***  chosen letters (different from the random image chosen). This now constitues our one shot task.<br>\n",
        "If our model is able to successfully return the label of the image from the support set which best matches the target image, then our model is successful. "
      ]
    },
    {
      "metadata": {
        "id": "PC8nJbNVAajT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_size = (28, 28)\n",
        "\n",
        "def make_oneshot_task(dataset, nway=20):\n",
        "  \n",
        "  # Choose nway random letters from dataset\n",
        "  letter_choices = np.random.choice(len(dataset), nway, replace=False)\n",
        "  \n",
        "  # Placeholders for our support dataset\n",
        "  X = np.empty((nway, 1, *img_size))\n",
        "  y = np.empty((nway), dtype=int)\n",
        "  \n",
        "  # Choose random letter from support set to be the target letter\n",
        "  random_target = np.random.choice(nway, size=1)\n",
        "  batch_letters = [dataset[k] for k in letter_choices]\n",
        "  required_letter = None\n",
        "  required_class = -1\n",
        "  for i, letter in enumerate(batch_letters):\n",
        "      # Choose random image from each letter\n",
        "      letter_index = np.random.randint(len(letter[\"images\"]))\n",
        "      X[i,] = letter[\"image_mat\"][letter_index]\n",
        "      y[i] = i\n",
        "      if(i == random_target[0]):\n",
        "        # Fetching a different image for our target image from chosen letter\n",
        "        required_index = (letter_index + np.random.randint(1, len(letter[\"images\"]))) % len(letter[\"images\"])\n",
        "        required_letter = letter[\"image_mat\"][required_index]\n",
        "        required_class = i\n",
        "  return X, y, required_letter, required_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nNgaQbfqLhVP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualizing one-shot tasks"
      ]
    },
    {
      "metadata": {
        "id": "tn8TJKBQM0W6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Helper Visualization function to ensure our datasets and task generation functions are valid."
      ]
    },
    {
      "metadata": {
        "id": "BOJ2UXXd_lqa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_oneshot_task(X, y, required_letter, required_class):\n",
        "  fig=plt.figure(figsize=(8, 8))\n",
        "  columns = 5\n",
        "  rows = X.shape[0]//columns + 1\n",
        "  for i in range(1, X.shape[0] + 1):\n",
        "    img = X[i-1]\n",
        "    fig.add_subplot(rows, columns, i).set_title(y[i-1])\n",
        "    plt.axis('off')\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(img.reshape(*img_size))\n",
        "  fig.add_subplot(rows, columns, X.shape[0] + 3).set_title(required_class)\n",
        "  plt.imshow(required_letter.reshape(*img_size))\n",
        "  plt.axis('off')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cRhUKYzx1mhc",
        "colab_type": "code",
        "outputId": "2ed1d76f-23dd-477f-bb60-3b84a6374e23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        }
      },
      "cell_type": "code",
      "source": [
        "# Visualize our training dataset\n",
        "visualize_oneshot_task(*make_oneshot_task(augmented_train_dict))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcoAAAHQCAYAAAAh0SohAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwOD/AbllwOz1L62QAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f41321fbac8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "bj29koiNJkxD",
        "colab_type": "code",
        "outputId": "be6fcd3a-8c1d-418e-fe97-8ec1a8cefac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        }
      },
      "cell_type": "code",
      "source": [
        "# Visualize our validation/dev dataset\n",
        "visualize_oneshot_task(*make_oneshot_task(augmented_dev_dict))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcoAAAHQCAYAAAAh0SohAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f411b5e55f8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "1_geXRMyJo4L",
        "colab_type": "code",
        "outputId": "aac61471-751e-4e73-8e57-093fcf3a5b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        }
      },
      "cell_type": "code",
      "source": [
        "# Visualize our test dataset\n",
        "visualize_oneshot_task(*make_oneshot_task(testing_dict))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcoAAAHQCAYAAAAh0SohAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xt0VeWZx/FfCCYSEi7SErXlogTw\nsmYcGRUQVyskmChprbXM8hLGy4y4SrsYCxQYx9GuquUyButo64XqFBVn7OjUKkhwAjouA3TwOh1R\nEAawXQKKQC4SuSXzR9fe7pR93nPbZ7/77PP9rMVaLzs5ez/nzTn7Pe9z3ktRd3d3twAAgK9etgMA\nACDKaCgBADCgoQQAwICGEgAAAxpKAAAMaCgBADDIq4Zy/fr1uuKKK1RbW6sbbrhBu3fvth1S7B05\nckQLFy7U6NGjqe8QrFmzRpdffrkuvfRSXX311dqyZYvtkGJt9erVuvzyy1VXV0d9h+iVV17R6NGj\n9Yc//MF2KCnJm4by4MGDmjVrlu666y6tXr1aEydO1B133GE7rNibMWOGysrKbIdREPbs2aP58+er\nsbFRq1atUn19vW6//XbbYcXWRx99pDvuuEM///nP1dTUpLq6Ot166622w4q9zs5ONTY2asCAAbZD\nSVneNJQbNmzQkCFDdPbZZ0uSrrzySrW0tKijo8NyZPE2Y8YMzZw503YYBaF3795qbGxUVVWVJOkv\n//IvtXXrVstRxZdT31/5ylckSePHj9f27dstRxV/999/v775zW+qb9++tkNJWd40lDt27NCQIUPc\n//ft21cDBgzQhx9+aDGq+Dv33HNth1AwBg0apK997Wvu/1999VWdc845FiOKt8GDB2vChAmSpKNH\nj+rXv/61qqurLUcVb5s3b9a6det0/fXX2w4lLb1tB5Cqzs5OlZaW9jhWWlqqgwcPWooIyJ3169dr\n2bJlWrZsme1QYm/ZsmX6+c9/rqFDh+pnP/uZ7XBiq7u7W3fccYduu+02nXDCCbbDSUve9CjLysp0\n6NChHsc+//zzvOq+A6lobm7W/Pnz9dBDD7lpWOTOddddpw0bNui6667TVVddpc8//9x2SLH09NNP\nq6qqSuedd57tUNKWNw3l6aef3iPN2t7ertbWVg0bNsxiVECw1q1bp7vvvluPPfaY/uzP/sx2OLG2\nbds2rVu3TpJUVFSk+vp6ffbZZ3xPmSNr1qzRmjVrNGHCBE2YMEG7du3Sd77zHW3YsMF2aEnlTUM5\nduxYffTRR3r99dclSb/85S81ceJERmQiNjo7O/X3f//3uv/++zVixAjb4cTevn37NHfuXO3Zs0eS\n9MYbb+jIkSM9xkIgOEuXLtX69evV0tKilpYWnXLKKXrmmWc0btw426EllTffUZ544olasmSJfvzj\nH6uzs1NDhw7VwoULbYcVa3v37lVDQ4P7/2nTpqm4uFjLli1TZWWlxcjiac2aNdq3b5/mzJnT4/iT\nTz6pL33pS5aiiq/zzz9f3/3ud3XDDTeoq6tLJSUluvfee1VeXm47NERMEftRAgCQWN6kXgEAsIGG\nEgAAAxpKAAAMaCgBADCgoQQAwMA4PaSoqCisOPJeUIOHqfPUBVHn1HfqqO9wcU8JX6I6p0cJAIAB\nDSUAAAY0lAAAGNBQAgBgQEMJAIABDSUAAAY0lAAAGOTNNlvInXTma9XX17vllStX5iKcvJerDXnW\nrFmT8u++9NJLkqTFixfnJBagkNCjBADAgIYSAAAD48bNLH2UunxcburAgQOSpP79+xuvf+jQIbdc\nUlLilm2nYaO6pFo6KdJ0TJo0KeXf3bdvnyRp0KBBgV0/qvUdpoULF7rl+fPn5/Ra+XhPyXcsYQcA\nQAYi3aP0C812LyaRfPz058Q8e/Zs99iSJUtSeozUs+dUU1MTcHTJFVoPx/t8bcQdRn03Nze75erq\n6qyvF2XJ6iIf7yn5jh4lAAAZoKEEAMAg7+ZRrlixwve4MzBFkgYOHBhWOHln5MiRxx1Llm6FPbka\nGAR/o0aN8j0+evRoSdILL7xg/N0PPvggN4HBKnqUAAAY0FACAGAQ6dRrstFa3hGw3pSIM3LJ9qjM\nKNqyZYvtEICEbI+efvHFF92y39cUXqRZ7fKOkHZcc801bvnjjz8O7Fr0KAEAMIh0jzIZ78Aeb+/T\n6Ul652HZnoMWRVVVVVk93m+e26xZs3x/lwFDiKrW1la37H1PeF/L3owVcrfwf7Z+85vfuOXx48cH\ndl56lAAAGNBQAgBgEOkl7LxLq91zzz2S7MeUSL4sN+WN00kzbdu2LeXHewdIpbNIdy6eV9SWsBs8\neLAkac+ePYGdP1F9x3UJOxsyfV5hvlezYSPOdK6Z6bKYzuOCnEPPEnYAAGSAhhIAAINIj3p10q1S\nz+41gvHmm29K6rkfpZ+tW7e65REjRvj+ThRTamF76qmnjjvGaOvo8o52dST6G0V1lCeSe/LJJ91y\nQ0NDRuegRwkAgEHkepRz5871Pc5C58HwzhNzeoqJvkz3myd51llnueVNmzblIsS85Qw+SNT7oFdi\nR7I9LisqKpKew3nfeLMriLZPP/1UknTSSSe5x+hRAgCQAzSUAAAYRC71eskll+T8Gk4KbPr06e6x\npUuX5vy6UeCdM+kMXHBSFFLP1JSThmVB+fR4B4TYmrdaSM4880y3nOzrgLVr17plvzQs7Bo2bFja\njykpKfE97qRcg9jTlR4lAAAGNJQAABhELvWaSDojBpPNV3PmZD7yyCPuMW+50Oa7DRo0yC2Xl5e7\n5fb2dkmp1b3f73iXIGT3EORKOqOvvelr5zXr3dd25cqVwQWGlHjT4el8vTBnzhxJPefb5wo9SgAA\nDCK9KHqiVWD8+M1vynRh3qAenwnbde73PLxzL70DUWyvOhPVRbq9+xg2NjZmdS0WRU8unfuE1xln\nnCGp5762ifjNo2RR9D/K9t6Z6FypniNZPVVWVrrljz/+OKNz0aMEAMCAhhIAAINIp14zFVQadd++\nfe4x74CXVK+ZiSil1JLFQurVnzeubPfKI/UarilTprjlZCnZXA8Cypd7ysKFC93yvHnzsrqm33sn\n0+VLnfm17733XkbX96JHCQCAAQ0lAAAGoc+jzDTNlw7nXJmmLvwe713uzjvnMp/t37//uGNRT41F\nmd/rLchdb7xfBSA3vClUv/eC9288evRo38cVGu/yn07qNQrSSbkmQ48SAAADGkoAAAxCS716N091\n5CrNF9RosZtvvtktP/zww2750UcflSQdO3YskOvYMmDAAEnSokWLLEeSv3KdvmaHi2jx/m29C0sg\n3uhRAgBgENo8SucyudoPzjvYxun99e79RYc5296fXzW1tra65f79+2d1fkeYg2mc+Pv162f8udPz\n/FPMo+y5f54z2MZZTD5uolDfhSRf5lF6lxB0lviLwjzKbK/vRY8SAAADGkoAAAwCS706Sz8lW/Yp\n0TmdpbrS2Y8skYMHD0qS+vbtm/W5/Nx0002ScjOf0nZqKtHOF8lEKU2SDtv1nU+o73DlS+rVuzvH\n7t27s7omqVcAAPIQDSUAAAaBpV6dlN2CBQvcY6WlpSk//sknn5QknXzyycbfq6mpSfmcYcqXNEkm\nnL+NlPjvY+PvQiowXNR3uPLlnsKoVwAAClws96O0IV8+/cUJPZxwUd/hypd7SpA9ys8++8wt52ow\npgk9SgAAMkBDCQCAQej7UQIA4MdGujUV9CgBADCgoQQAwIDUKwAgY9u2bXPLcd3blh4lAAAGzKMM\nSL7MeYoT5vWFi/oOF/eU8DGPEgCADNBQAgBgQEMJAIABDSUAAAY0lAAAGNBQAgBgQEMJAIABDSUA\nAAY0lAAAGNBQAgBgYFzCLkr+8Ic/qLa2VkOGDHGP/fmf/7kWL15sMap427Nnj+bPn6+dO3eqb9++\nuv3223X++efbDiu2mpqa9NOf/rTHse3bt+uNN95QeXm5paji7dlnn9Wjjz6q7u5unXzyybr99tt1\n2mmn2Q4rtp577jktXbpUn332mc4//3zdfffdKikpsR1Wct154ve//333xIkTbYdRUK6//vruxx57\nrLu7u7t7/fr13TNnzrQcUWFZuXJl9/e//33bYcTW1q1buy+44ILu3bt3d3d3d3c/9dRT3VdddZXl\nqOJr8+bN3RdccEH3Rx991N3V1dU9a9as7gceeMB2WCkh9Qpfu3bt0rvvvquGhgZJ0rhx43TfffdZ\njqpwHDp0SPfdd59++MMf2g4ltrZt26bhw4ersrJS0h9f4x988IHlqOJrw4YNGjdunE455RQVFRXp\nuuuu00svvWQ7rJTkVUPZ0dGhGTNmqK6uTn/zN3/TYx80BOv999/XV7/6VTU2Nqq2tlYNDQ3atGmT\n7bAKxjPPPKMxY8Zo6NChtkOJrXPOOUcffvihtmzZou7ubr300ku68MILbYcVW0VFRerq6nL/X1ZW\npg8//NBiRKnLm4ayb9++qq+v16233qoXX3xREyZM0IwZM3T06FHbocVSW1ubtmzZovPOO0+rV6/W\nN7/5TX3/+9+nvkPQ1dWlxx57TDfeeKPtUGKtsrJSs2bN0re+9S1dcMEFWr58uebMmWM7rNgaP368\nWlpatGXLFh09elTLly/XoUOHbIeVkrxpKAcOHKjbb79dX/3qV9WrVy/dcMMN2rt3r3bs2GE7tFiq\nqKjQoEGDVFNTI0maOnWqWltbqe8QvPXWWyorK9PIkSNthxJrmzZt0oMPPqjm5mZt3LhRs2fP1ne/\n+93A9oFET1VVVfrHf/xHzZo1S3/1V3+lqqoqVVRU2A4rJXnTULa2tur3v/99j2NdXV3q3bu3pYji\n7dRTT9Vnn33mpkqKiorUq1cv9eqVNy+ZvPXKK6/o61//uu0wYm/9+vU699xzdeqpp0qSLrvsMm3d\nulX79++3HFl8XXHFFVqxYoX+4z/+Q6NGjdKoUaNsh5SSvLnr/e53v9N1112nffv2SZJ+9atf6ZRT\nTukxXQTBGT16tAYPHqx///d/lyStWrVK/fr14zuzELz//vsaMWKE7TBi77TTTtNbb73lNoz/9V//\npS9/+csaOHCg5cjiaefOnbr88svV1tamI0eO6KGHHtK3v/1t22GlJG+6YxdddJGuueYaXX311Soq\nKlJlZaXuv/9+FRcX2w4tloqKivTP//zPmj9/vh555BENGjRI9913Hz34EOzevVtf+tKXbIcRe5Mm\nTdK7776rq666SpJUXl6un/70pyoqKrIcWTwNGzZM1dXVuvzyy1VUVKQpU6boiiuusB1WSvJmwQEA\nAGzIm9QrAAA20FACAGBAQwkAgAENJQAABjSUAAAYGMf6M0w6dUENHqbOUxdEnVPfqaO+w8U9JXyJ\n6pweJQAABjSUAAAY0FACAGBAQwkAgAENJQAABjSUAAAY0FACAGBAQwkAgAENJQAABjSUAAAYsF09\nkIX9+/e75QEDBkhiyTAgbuhRAgBgELkeZaYLAfMpPhwjRoxwy9u2bbMYSTQ4vUhJmj17tsVIAOQK\nPUoAAAxoKAEAMCjqNuQ6w0xnBrX3miRt3bpVkjRy5MjAzpmMzb3jgqy7dBw+fNgtHzx4UJI0cODA\n0K5va3/EYcOGueUdO3akfK4DBw5Ikvr375/2NVMxffp0t7x06dLAz89+lOFiP8rjrVmzxi1PmjRJ\nUrDPj/0oAQDIAA0lAAAGVke9+nVzM+1Ge0cc3nPPPZIKZ4RmojoLahTmnXfe6Zb79Onjll9++WW3\nXFtbKyl5uigOaaDevTN728ydO1eSVFFREWQ4rlykWwE/s2bNcsuNjY1uOdfv73vvvdctO6nX5uZm\n91hNTU1OrkuPEgAAg9AH84TxSWTDhg2SpLFjx+bk/H6i/MV7sti8Pc8lS5YYH5/sdxM9bt68eZKk\nxYsXm4NNg63BJd5MhTNwLNNz5ZNCGMyT6DkuX75cktTQ0GA9lnTlos7r6+vd8gsvvHDcz8866yy3\n/N577wV+fcm/fqqqqtxyJllEBvMAAJABGkoAAAximXp1JHpqNtKbqcp1bK2trcf9PJ15femkXnPN\nViowzNdVlEQh9RrkAEBnUNaRI0d8f+6ds5erQSImUb6nJHLs2DFJUq9eX/TBbH3t5Qya6+joyPpc\n9CgBADCgoQQAwCByu4cEyZtmzNWyYfnGu9uFH+8cv7a2tuN+bjvdGjUnnHCC7RAKijeN56TJvOmy\nZGk+v/1DERxnRPj27dvdY+n8fdLhLJsZBnqUAAAYxLpH6f3EaGvh8HzT3t5uO4TI8g5Ecxw9etRC\nJJC+6J00NTW5x9J5nzuLyHtXNOI+kR3vJgG54B1g5V0lzOHNiKUziCcZepQAABjQUAIAYBB66tU7\nGMQ7j7K8vFxSsN1lZ/8/fCGd5eyAfFBXV2c7BPwJ76AdZ/nKIDgLoXt57/NBth9e9CgBADCgoQQA\nwCAyo16d0ZZBzrPxzp0s5JSit06HDx/ult9++21JPUePedPhyJ53pxFHnPdGBf7UokWLsnq8957l\ndfjwYUnSwIEDszp/KuhRAgBgYLVH6d0p3pmPFsSC037nYEWZP/LOc0q2MonTG/LuuYj0pFN3cV9U\nHciEd5Ufr9LS0tBioEcJAIABDSUAAAZWU6/O3mXSF2mnKVOmuMdGjx6d0XmdxXIrKyuziA6kXLPn\nl071LoU3efJkt5xsjuvatWvdcnV1dQDRAdHivef4DYSz9fUEPUoAAAxoKAEAMCjqNuR7GIWXuqB2\nHYhSnfs9p6jHl65Mno/3uosXL3bLQS3V1dzc7JbTSbHm+m9jq77D5H2O3r+n9+9sI5Zs5FOdey1f\nvlyS1NDQYD0WepQAABjQUAIAYEDqNSBxTJO0trZKkvr16+ceSxafd7mpvXv3uuVcrOpvKxU4bNgw\nt+xdwCFKf7tciFvq1W9BDWfEvCT17ds39Ji84nhPcQSxsEwukHoFACAD9CgDEsdPf8XFxZK+WF5Q\nkqZPn+6Wly5detxjEtWD0ztNtmxeOqLQw/H2Lnfu3JltOJEWhfrOVrLnYDs+rzjeU/yeU0VFhVvO\n1X6SqaJHCQBABmgoAQAwIPUakDimSRw33XSTW37kkUfccrJY6+vr3fILL7yQ0mPSEYdUYD7J1/r+\n9NNP3fJJJ51kNZZ0xOWeEvV0qxepVwAAMkBDCQCAAanXgMQlTZJMojSWn23btrllZ84aqdf8RX2H\nKx/vKfv375eUeHR71P/+pF4BAMiA1f0okX8GDRrke9zvk6R3PzlngWMA8eLXC0s23zrf0KMEAMCA\nhhIAAAMG8wQkH794z3cMLgkX9R2uKN5TnnzySUnStddem/Nr2cBgHgAAMkBDCQCAAaNeAQCSpObm\n5uOOVVdXH3ds9uzZbnnJkiU5jSkK6FECAGBAjxIACtiUKVPcsl/v0SvfB+tkih4lAAAGNJQAABgw\njzIgUZzzFHfM6wsX9R0u7inhYx4lAAAZoKEEAMCAhhIAAAMaSgAADGgoAQAwoKEEAMCAhhIAAANj\nQ9nd3W393+HDh7VgwQKNGjVKu3btco//y7/8i+rq6nTJJZfo1ltv1aFDh6zGGZSo1vfevXt1/fXX\nq6amxnqMQda57edgqvMHHnhAtbW1uuSSS/R3f/d3amtro75zWN/33Xdfj/pubW3N+/qOQp0nqm/n\n38KFCzVx4kTrcZrqPPI9yhkzZqisrKzHsbfffluPP/64nn76aTU1Nam9vV1PPPGEpQjjxa++Dxw4\noIaGBo0aNcpSVPHmV+dNTU1qamrSM888o1WrVqmoqEi/+MUvLEUYL371vWLFCq1bt07PPfecVq1a\npa6uLj300EOWIowXv/p2vP/++747lkRNXjSUM2fO7HGsqalJl112mfr166eioiJdeeWVampqshRh\nvPjVd1FRkX72s59p0qRJlqKKN786HzFihBYsWKDy8nL16tVL5557rj744ANLEcaLX31XVVXpRz/6\nkU488UT16tVLF1xwgbZv324pwnjxq29J6urq0o9+9CPdcsstFqJKT+R3Dzn33HOPO7Zjx44eN+0h\nQ4bo//7v/8IMK7b86rt///7q37+/PvnkEwsRxZ9fnY8cObLH/1999VWdf/75YYUUa371fcYZZ7jl\n9vZ2NTU16fLLLw8zrNjyq29J+rd/+zeNGjVK55xzTsgRpS/yPUo/nZ2dKikpcf9/4oknqrOz02JE\nQO48+OCD+vTTTzVt2jTbocTe7NmzddFFF2no0KH61re+ZTuc2Prkk0+0bNmyHhtAR1leNpR9+vTR\n4cOH3f93dnYmzIED+ayxsVH/+Z//qUcffZTXeAgaGxv13//93yorK9MPf/hD2+HE1oIFC/S9731P\n/fv3tx1KSvKyoTz99NO1c+dO9/87d+5UVVWVxYiA4N1///1688039fjjj+ukk06yHU6srV+/3v0O\nuLS0VFOnTtVrr71mOar4evnll7Vo0SJNmDBB3/nOd7Rr1y5NmDChRwcoSvKyobz00ku1cuVK7d27\nV0ePHtXjjz/eY5duIN/97//+r5577jk99NBDKi8vtx1O7L3xxhtauHChe6N++eWXNXr0aMtRxddb\nb72llpYWtbS06JlnntEpp5yilpaWHl+pRUmkB/Ps3btXDQ0N7v+nTZum4uJiLVu2TDfeeKOuvfZa\ndXd368ILL9TVV19tMdJ4SFTfN998sx5++GF9/vnn2rt3r+rq6lRZWally5ZZjDYeEtX5eeedp/b2\ndk2dOtX92Ve+8hU9+uijNsKMDdM95ZNPPtE3vvENSdLJJ5+su+66y1aYsWGq78rKSouRpce4cTMA\nAIUuL1OvAACEhYYSAAADGkoAAAxoKAEAMKChBADAwDg9pKioKKw48l5Qg4ep89QFUefUd+qo73Bx\nTwlfojqnRwkAgAENJQAABjSUAAAY0FACAGBAQwkAgAENJQAABjSUAAAYRHqbLQBmzc3NvsdrampC\njgTIDe9r3Nbrmh4lAAAGNJQAABiQegVCctNNN0mSKioq3GONjY22wsl7U6ZMkSStWLEio8cfPHhQ\nklRZWeke6+joyD6wmPEu69bU1CRJuvTSS3N+3UWLFkmSqqurc36tZOhRAgBgUNRtWHmXxXRTF+cF\njEeMGOGW9+zZ45Ztf/rOh0W6k8XY2trqlgcMGJDTWLIVtfr2iyfZ+adPn+6WH374YePvVlVVueVt\n27alGV32onJPOXDggFvu37+/JGnDhg3usfHjx2d1fq/y8nK33N7eLkkaPny4e2znzp2BXcsPi6ID\nAJABGkoAAAxIvfrYv3+/pJ6psGR1EZU0SbZmz57tlu+5556szmU7rZkKGzHa/htnKmr17cQT5Dm9\nKcWxY8ce93NvGnLgwIGBXddPFO8pTv146ybIOrH9fiH1CgBABmgoAQAwyCr1GlRqIB8USurV+zyc\neWZ9+/ZN+riRI0dKkrZs2eIeI/X6Rb1IX9RNOtf0Psd9+/a55UGDBgUQXXqiUN/eGFavXi1Jqqur\ny+qcqXC+kkj0dUQuXkdRvqecddZZbvndd9/N6prz5s1zywsXLszoHEEh9QoAQAYiN5hnzZo1bnnS\npElWY0lHlD/9pcP7PJwv6VP5gp4eZXLJBp/4PZ+tW7e6Ze+8vnR6+0GJQn17Y3B6eUuWLMnqnEHE\n4vydvBmEIM+fDduDYdLJxnnv/zYWQKdHCQBABmgoAQAwYFF0IGSzZs1yy36LoidKVQU5xzWfJEqH\n2Uq5Op566im3fM0110jqudzajh07Qo7IPue16/2becvOz71zL72iuo8qPUoAAAxoKAEAMIjcqNfm\n5ma37N2HzPZo0GTycYSan0xHvTo7jHhHaTLqtae2tjZJPfejzDSWXCzfluo1s5FJvImua/u94pWL\npdfieE/x4/1KwXY6nVGvAABkIPTBPIW0mg/g1a9fP0nBvge8e4Xa2DMRfzRnzhxJhTXIKlXeHq3f\na3/z5s1hhpMRepQAABjQUAIAYJDT1KtfNzvZws6JBvPcdNNNkqSlS5emfH3vXJ3+/fu7ZWeZpKjO\n2UG8eVOk3mXpkNyiRYtsh+Br8uTJtkOIrDPPPPO4Y4cPH3bLK1ascMu2Bx4lQo8SAAADGkoAAAxy\nOo/Sm/qcO3euJOmRRx4xPibZiMDevb/IFh87diyjcy1fvlyS1NDQYHx8OqI45ynVmLzz+trb290y\n8yijxW8Ju0KYR5noKxTnddvR0eH7uNbWVklfjDbO9Ppe3lHG3te637FsdxKJ4j0lE8nmZh86dMgt\nl5SUuGX2owQAIE9EZmUep3fYq1fqbff06dPdst8gnzBX9IjKpz9vL9upy0Tn9PvE7UWPMlq8z9dZ\n5cfbwwrz+pkKcrUavz05nde05P+6ztVqOXG+p2Qi0xXWcrHCUTroUQIAkAEaSgAADKzuR7l+/Xq3\n7KQJnTmOkn+X3buXn3dgULJBQoXCm7o+++yzjb/rpO1YVjD/hJlyjRK/5dASvX6dOapBLu2XT6l6\nm7z3bu89PZlkf19b9U+PEgAAAxpKAAAMQk+9rlq1yi2PGzfOLVdWVkqSnnrqKePjvfuVJdu7LNH8\nq0KxadOmlH5v3rx5bjmqS4QVqvr6etshRJaT0vOm+Zw50hK7qURFpkuFOmlWb+rVVhqWHiUAAAY0\nlAAAGIS+4ECyrnOmE1X9DB8+3C1v3779uJ+fdtppbnnHjh1pn98rKpODs01NJFtuKhEWHMgN73P0\n7rhQWlpqNZZM5WKJxrVr17rHvPcMh3d0vffrnmRsvzaick9Jxtao+Q0bNkiSxo8fH9g5WXAAAIAM\nhDaYx7vwba45y3t5F/v249fLlOx/kgyK8+nI2xPp7Oy0FQ6yNGTIENsh5JVMlsX08vYuurq6JEnF\nxcXZB4ZApJMdyBY9SgAADGgoAQAwCC316uwzliyt6f0y3m+/t1Q4KVdnCSupcOZUeevXWe6vsbHR\nPebd7w355eOPP7YdQl7xS7nOnz/fLSebM3zmmWe65VTnJBci7z2nvLxcUuI9QoMU5hJ39CgBADCg\noQQAwCC0eZTO/Mhkyxl5w/GuOp/OMkipzuu5+eab3XK2u4/ky5ynXGEeZXASLb1oO9ao1bcTj/dr\nFe/XLY5Ec7Md3tHx3pRhU1PV25hTAAARQ0lEQVSTW66trZUUjU2E02X7dZNrQW6mzTxKAAAyENpg\nnkwXxs2EsyLP22+/7fvzAQMGSGIPS0TfwYMHbYcQWX6LovtJdO9xeg8XX3yxe+y2225zy2PHjnXL\nmQ4sRO55V2DbuXNnTq5BjxIAAAMaSgAADEJfFD2ZIAbzpHqNZM/PO6jCy0nd+p0zW3H/4j1IURtc\nEndRre9M59Elez7OottSsAtvp4p7SvgYzAMAQAZoKAEAMIh06jVX11+1apUkqa6uLuXHJIuFNEn4\nopoKjKuo1ret/RD9eL+uSWUfVxPuKeEj9QoAQAYi16PMV3z6C19UezhxlQ/17V2FJ1VhzvFOB/eU\n8NGjBAAgAzSUAAAYkHoNCGmS8OVDKjBOqO9wcU8JH6lXAAAyQEMJAIABDSUAAAY0lAAAGNBQAgBg\nQEMJAIABDSUAAAbGhrK7u9v6v8OHD2vBggUaNWqUdu3ape7ubj377LMaM2aMamtr3X9PPPGE1TiD\nEsX67u7u1saNGzVlyhRVV1dr2rRp2r17t/VY41Dfiep80aJFPV7fX//613XFFVdQ3zmq76NHj+qu\nu+7SJZdcorq6Os2fP18dHR15X99RqHO/+j5y5IgWLFig2tpaXXzxxVq6dKn1OE11Hvke5YwZM1RW\nVnbc8cmTJ6upqcn919DQYCG6+PGr746ODt1yyy2666671NzcrIsuukgrV660FGH8+NX53Llze7y+\nL774Yl1xxRWWIowXv/p+9tlntWnTJr3wwgtauXKlDh8+rEceecRShPHiV9+/+tWv9M477+g3v/mN\nnn/+eT377LN6/fXXLUWYXF40lDNnzrQdRsHwq+/m5madffbZ+ou/+AtJ0vTp03XjjTfaCC+Wkr3G\nt2zZoo0bN+rqq68OMar48qvvLVu2aMyYMSopKVGvXr10wQUX6IMPPrAUYbz41fe6detUX1+v0tJS\nVVRU6Nvf/rZWr15tKcLkIt9Qnnvuub7H33vvPU2bNk21tbW69dZb1d7eHnJk8eRX35s3b9bAgQP1\nve99T7W1tfrBD36gffv2WYgunhK9xh0PPPCA/vZv/1a9e/cOKaJ486vvcePG6dVXX1Vra6sOHTqk\nl19+WRMmTLAQXfz41XdRUZG6urrc/5eVlenDDz8MM6y0RL6h9DN8+HBVV1frwQcf1HPPPaeOjg79\n5Cc/sR1WbLW1tem1117T3LlztWLFCpWUlFDfIdm5c6feeecd1dfX2w4l1mpqanTGGWdowoQJGjdu\nnNrb2zV16lTbYcXWhRdeqGeeeUZtbW3av3+/nn/+eR06dMh2WAnlZUM5ZswYzZw5U+Xl5erTp49u\nvvlmvfLKK7bDiq2KigqNHz9ew4YN0wknnKC//uu/VktLi+2wCsKLL76oyZMn64QTTrAdSqw9/vjj\n2rdvnzZu3KiNGzdqxIgRfBjMoalTp+rCCy/U1KlTNXPmTF144YXq16+f7bASysuGcteuXT1Sf8eO\nHSMtlUOnnnpqj9R2cXGxiouLLUZUOF555RV97Wtfsx1G7LW0tGjy5Mnq06ePevfurbq6Om3cuNF2\nWLHVu3dvzZs3T6tXr9YTTzyh4uJijRo1ynZYCeVlQ/mv//qvuu2223TkyBEdO3ZMTzzxhC6++GLb\nYcVWTU2NNm7cqM2bN0uSnn76aY0fP95yVIVh8+bNGjFihO0wYu+0007Tq6++qqNHj0r64weUkSNH\nWo4qvp5//nn94Ac/UFdXl/bs2aNf//rX+sY3vmE7rISM+1HatnfvXnfax/bt2zV06FAVFxdr2bJl\nuvfee/Xmm2+qqKhIY8aM0a233qqKigrLEec3U32/8847+qd/+icVFRVp5MiRuvPOO3XSSSdZjjj/\nmeq8tLRUY8eO1e9+9zuVlJRYjjQeTPW9ePFi/c///I969eql4cOH68c//rEqKystR5zfTPV95513\natOmTerdu7duueUWXXbZZZajTSzSDSUAALblZeoVAICw0FACAGBAQwkAgAENJQAABjSUAAAYGGfp\nFxUVhRVH3gtq8HC+13mienCWYAty15Eg6jzf6ztM1He4uKeEL1Gd06MEAMCAhhIAAAPjggN02VNH\nmuSPDhw44Jb79Onjlp2VZbw/HzhwYFbXIhUYLuo7XNxTwkfqFQCADNCjDAif/sxmz54tSbrnnnvc\nY9k+V3o44aK+w8U9JXz0KAEAyAANJQAABqReA0KaJDXeeqqqqnLL27Zty+pcmcpFfTtpZinYVHMy\na9asccuTJk0K/JpRre+44p4SPlKvAABkgIYSAAAD4xJ2QC5t3brVLZMeyl51dbVbZj92IDj0KAEA\nMAi9Rzlr1iy33NjYeNzP6Vkg30WpNzds2DC3vHPnTouRAPmLHiUAAAY0lAAAGISeel26dKlb9ku9\nelOzS5YsCSUmIAxOGjTMFOiOHTvcMl9rAJmhRwkAgAENJQAABlaXsPNeevHixZKkefPm5fSaucJy\nU6lJVE+ZPO98WFLNb8m+TJbrSxe7taRu+fLlbrmysjKw83rntdbX10uSVq5cmfLjuaeEjyXsAADI\nQOiDeVatWuV7PF97ksCf8g5I8wqjJ1mIojRv1WvDhg1uOZ2eZKHJhwGc9CgBADCgoQQAwCD01Gtd\nXZ1bJhUFR3l5uSSpo6PDciS546QIGVwRrFztuVnIf6cRI0a4Ze/mBRUVFZKCfZ9659NPnTrVLY8f\nPz6wa2SLHiUAAAY0lAAAGISWen3yySePOzZo0KCwLg8LDh065JZLSkqMv9ve3i6pMNJdiUZpHjhw\n4LhjAwcOzHU4QMpy8T49+eST3fLu3bsDO2+Q6FECAGAQWo/y2muvlSStXr3aPVZbW+uWGeiQf5zV\nRiTphRdeSOl3V6xY4fvzQvi7+z1HZwUdqecqOo5kcwTXrFnjlmtqarKIDviCd6Cl93XrDOwJctDT\nnj173LK3fYhSm0CPEgAAAxpKAAAMrM6j9C5A7HyJyzym1CVaKi0ofvuFJpPO38xZJLxQOK/ts846\nyz3mreNk9e1Ndf/DP/yDpJ4Lb3vfO3PmzMku2ALiN4gK/pz3rPe1FuQ929s+OOd15lhL9uZZ06ME\nAMCAhhIAAIOc7ke5f/9+tzxgwICUzhnkfoVhsrF3XK53TcjFvD6//RmlzJYzjOr+iN6UuDed2tXV\nJUnq1Sv1z6fp1FGy+mA/Sn/e13n//v3dsu1Y82U/ylx9Xeb3/MN8Ll70KAEAMMjpYB6nFyn1nC9m\n4h3g451fc+zYMUlScXFxQNHlv6A+XTU3N7tl7+AQVoUJlt9r1ztQwVn1xMu7IPXhw4fdcmlp6XG/\n6309OO83v7mZcfPZZ5+55bKysuN+nmiwjlNfzkLfyJ6zmHrcNrygRwkAgAENJQAABqHNo2xqakrp\n9z7++GO37J1vtmnTJknMs8wF79JnuR4ghJ6888L8Xs9Tpkxxy97l/2wMdIiauXPnSuqZbvX7iieT\n+cCAFz1KAAAMaCgBADAIfB6lM+pJ6jliL9u00ODBgyX1HAnrZTvtlC9znpLJdWq7kOdRBnmtZO8H\nZxcG7w49cZtH6cQT5A4q3ueYznlzsdNFvtxTcvU1APMoAQDIEzSUAAAYBD7q1ZtuDZIzGtbb9c7V\nCvZA1Pm9H5YvX+6Wr7nmmuMeE7f3SC6ew9q1a92yd/GN1tZWSdKYMWPcY4kWgyg0bW1tbrlfv34W\nI8kdepQAABiEvh9lkLyf/iZNmmQxEsC+a6+99riytxfZu3dev91D4e1FLly40C3PmzdPUuKMmd+S\ngoUiUS87TuhRAgBgQEMJAIBBYLkY79wxR64HDHjTJIkG9oQVCxB1zg48SM38+fN9y+gpbjuF+KFH\nCQCAAQ0lAAAGgaVeba/QT2oVAKLBu1tUXV1dVufyLnVpCz1KAAAMAp9YRc8OAAqTc/8Pcl/bKAwW\nokcJAIABDSUAAAasaQUAyJl0FuMPMmUbJHqUAAAY0FACAGBQ1G3o6zKCNXVBpQziXufepQ6XLFmS\n1bmCqPO417e3jrzz0TIZSUh9hysu95RMn4eNuBPFSo8SAAADepQBicunv3xCDye5wYMHu+WPP/44\nq3NR3+GK8z2lubnZ93hNTU3IkfREjxIAgAzQUAIAYEDqNSBxTpNEFanAcFHf4eKeEj5SrwAAZICG\nEgAAAxpKAAAMaCgBADCgoQQAwICGEgAAAxpKAAAMjPMoAYRr/fr1Wrx4sQ4ePKhTTz1VCxYs0Mkn\nn2w7LKCg0VACEXHw4EFVV1frF7/4hc4++2w9/vjjamlp0cMPP2w7NKCgkXoFImLDhg0aMmSIzj77\nbEnSlVdeqZaWFnV0dFiODChsNJRAROzYsUNDhgxx/9+3b18NGDBAH374ocWoANBQAhHR2dmp0tLS\nHsdKS0t18OBBSxEBkGgogcgoKyvToUOHehz7/PPP1bdvX0sRAZBoKIHIOP3003ukWdvb29Xa2qph\nw4ZZjAoADSUQEWPHjtVHH32k119/XZL0y1/+UhMnTlRZWZnlyIDCxvQQIEJ++9vf6u6771ZnZ6eG\nDh2qhQsX6stf/rLtsICCRkMJAIABqVcAAAxoKAEAMKChBADAgIYSAAADGkoAAAx62w4AsKmoqMh2\nCHmDAfIoVPQoAQAwoKEEAMCAhhIAAAMaSgAADBjMA+TQ/v37jzs2cOBAC5EAyBQ9SgAADGgoAQAw\nYPcQFLRcz6NM5+0V9Tmd3CpQqOhRAgBgQEMJAIABo16BHEqWTvWmM5OlNqOemgXiih4lAAAG9CgB\ni/x6iVu3bnXLQ4YMccveHie9SyA89CgBADCgoQQAwIDUK2DR9OnT3fKiRYskscQdEDX0KAEAMKCh\nBADAgCXsUNBsjx71e/vZjikRbhUoVPQoAQAwYDAPEBEHDhwI/JzeXmBVVZVb3rZtW+DXAuKKHiUA\nAAY0lAAAGDCYBwXN9sCZXC9Ll+jtncm1uFWgUNGjBADAgIYSAAADRr0CEeGkNr2jX7Ndzm7UqFFu\necuWLVmdCyhU9CgBADBgMA8Kmu3BPF5+b0Xv3pQjR45M+5wVFRVuua2tzS3X19dLklauXJlVfEAh\noEcJAIABDSUAAAYM5gEiwi8N7E13esuzZ8+WJC1ZssR4zvb2dt/jo0ePlpRe6hUoVPQoAQAwoKEE\nAMCAUa8oaFEa9ZrMwYMH3XKfPn2O+3my55Lt3pfcKlCo6FECAGBAjxIFLZ96lF5TpkyRJK1YscI9\n1tnZ6ZbLysqOe8ywYcPc8o4dOyTRowRSQY8SAAADGkoAAAxIvaKg5Wvq1eFdQL1///5uOcy9LYG4\no0cJAIABDSUAAAakXlHQ8j316uV9K5N6BYJDjxIAAAMaSgAADNg9BAhItkvEAYgmepQAABjQowSy\n8MEHHxx3jF4kEC/0KAEAMKChBADAgNQrkKbm5ma3XFVV5ZbnzJljIxwAOUaPEgAAAxpKAAAMWMIO\nBS2dEaqzZs2SJDU2NrrHfvvb37rlcePGGR8/ffp0SdJrr73mHtu0aVPK10+GJeyA3KBHCQCAAT1K\nFLR0el7OW6Wrq8s9VlxcnPLjnTmX3gFA2fb82tra3HJFRUVg5/XDrQKFih4lAAAGNJQAABgwjxIw\n8Es3ppNu9Ro5cuRx58x0AM7w4cMl9Uy3Llq0KKO4AJjRowQAwICGEgAAA0a9oqD5pTv379/vlgcM\nGGD83Wz5vf3Wrl2b9HGTJk067liudy3hVoFCRY8SAAADepQoaFHaO3L9+vWSkq/wI0nLly+XJDU0\nNOQ0Ji9uFShU9CgBADCgoQQAwIDUKwpalFKvUcetAoWKHiUAAAY0lAAAGNBQAgBgQEMJAIABDSUA\nAAY0lAAAGNBQAgBgwH6UKGjMDQSQDD1KAAAMaCgBADCgoQQAwICGEgAAAxpKAAAMaCgBADD4f84w\nxjQkYD9SAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f41176c1080>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "QESv_nGFOfot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create a PyTorch `Dataset` class that allows us to leverage multiprocessing while loading our data between batches."
      ]
    },
    {
      "metadata": {
        "id": "qsNZHRbbX1it",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "class OneShotDataset(Dataset):\n",
        "  \n",
        "  def __init__(self, data_dict, iterations=1000):\n",
        "    self.data_dict = data_dict\n",
        "    self.length = iterations\n",
        "  \n",
        "  def __getitem__(self, index): \n",
        "    support_images, support_labels, target_image, target_label = make_oneshot_task(self.data_dict)\n",
        "    return (torch.from_numpy(support_images).float(),\n",
        "            torch.from_numpy(support_labels).long().unsqueeze(-1),\n",
        "            torch.from_numpy(target_image).float(),\n",
        "            target_label)  \n",
        "  def __len__(self):\n",
        "    return self.length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_MQ9BQprPC0M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Helper function that will allows us to evaluate our model"
      ]
    },
    {
      "metadata": {
        "id": "mGQOEW4VLbuT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataset_dict, iterations=1000, batch_size=20):\n",
        "  model.eval() # Set the model in eval mode so as ensure Dropout and BatchNorm layers operate in Evaluation mode\n",
        "  \n",
        "  # Set up our dataloaders\n",
        "  dataset = OneShotDataset(dataset_dict, iterations=iterations)\n",
        "  dataset_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  num_workers=4,                                                  \n",
        "                                                  shuffle=False)\n",
        "  \n",
        "  # Run in no_grad mode to speed up inference \n",
        "  with torch.no_grad():\n",
        "    # Placeholders to accumulate the metrics across batches\n",
        "    total_accuracy = 0\n",
        "    total_loss = 0\n",
        "    for iteration, (support_images, support_labels, images, labels) in enumerate(dataset_loader):\n",
        "      # Move Data to GPU\n",
        "      support_batch_torch = support_images.to(device)\n",
        "      support_labels_torch = support_labels.to(device)\n",
        "      target_images_torch = images.to(device)\n",
        "      target_labels_torch =labels.to(device)\n",
        "\n",
        "      # Perform inference via our model\n",
        "      logits, predictions = model(support_batch_torch, support_labels_torch, target_images_torch)\n",
        "      # Calculate loss and accuracy\n",
        "      loss = F.cross_entropy(logits, target_labels_torch)\n",
        "      accuracy = torch.mean((predictions == target_labels_torch).float())\n",
        "      \n",
        "      # Accumulate the metrics\n",
        "      total_loss = total_loss + loss.item()\n",
        "      total_accuracy = total_accuracy + accuracy.item()\n",
        "      \n",
        "    accuracy = total_accuracy/(iteration+1)\n",
        "    loss = total_loss/(iteration+1)\n",
        "    return accuracy, loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o9zHIJx9KdQG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Baseline KNN Accuracy"
      ]
    },
    {
      "metadata": {
        "id": "yI4Qj_AqITSs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def evaluate_one_shot_performance(dataset, nway=20, num_tests=300):\n",
        "  correct = 0\n",
        "  for i in range(num_tests):\n",
        "    support_set, support_labels, required_letter, required_label = make_oneshot_task(dataset, nway=nway)\n",
        "    images = np.insert(support_set, 0, required_letter, axis=0)\n",
        "    # Baseline model with just l2 Distance between images\n",
        "    embeddings = images.reshape(nway+1, -1)\n",
        "    neigh = KNeighborsClassifier(n_neighbors=1)\n",
        "    neigh.fit(embeddings[1:], support_labels)\n",
        "    prediction = neigh.predict(embeddings[:1])\n",
        "    if(prediction[0] == required_label):\n",
        "      correct = correct + 1\n",
        "  print(f\"Accuracy - {correct * 100 / num_tests}%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vANi98TkJ1Fg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evaluate_one_shot_performance(testing_dict, nway=20, num_tests=500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Axa3vKahH1uW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Training our model"
      ]
    },
    {
      "metadata": {
        "id": "uAW_F_oQHxd1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "mNLyMZRzJ2Tk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 60 #@param {type: \"slider\", min: 1, max: 100}\n",
        "batch_size = 20 #@param {type: \"slider\", min: 20, max: 200}\n",
        "iterations = 10000 #@param {type: \"slider\", min: 500, max: 10000}\n",
        "lr = 0.0001 #@param [\"0.1\", \"0.01\", \"0.001\", \"0.0001\"] {type:\"raw\", allow-input: true}\n",
        "embedding_size = 64 #@param {type: \"slider\", min: 64, max: 512}\n",
        "dropout_probality=0.1 #@param {type: \"slider\", min: 0, max: 1, step: 0.1}\n",
        "use_fce = True #@param [\"True\", \"False\"] {type:\"raw\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3uAlr5cRIP6s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Setting up logging for our training"
      ]
    },
    {
      "metadata": {
        "id": "3OKBnMEtQalU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will now setup an experiment via comet.ml library. This will allow us to keep track of multiple runs of our code and the hyperparameters used in each iteration.\n",
        "It also generates the loss and accuracy graphs over time.\n",
        "We are mostly interested in logging the\n",
        "\n",
        "1.   Train loss\n",
        "2.   Train accuracy\n",
        "3.   Dev loss\n",
        "4.   Dev Accuracy\n",
        "5.   Model Performance (Test Accuracy)\n",
        "5.   Model loss (Test loss)\n"
      ]
    },
    {
      "metadata": {
        "id": "7bKU4FwbWAau",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pytorch_experiment = Experiment(api_key=\"SCmpLSERj7defIsfWhQcs7D4E\",\n",
        "                        project_name=\"PyTorch Matching Net\", workspace=\"ramesharvind\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qQuOLUOgL_HJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pytorch_experiment.log_multiple_params({\"Epochs\":epochs, \"Batch Size\": batch_size, \"Iterations Per Epoch\": iterations, \"Learning Rate\": lr, \"Embedding Size\": embedding_size, \"Dropout\": dropout_probality, \"Using FCE\": use_fce})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lOEJBsjiH6ls",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Training Loop"
      ]
    },
    {
      "metadata": {
        "id": "jvioaFnoJa-z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "matching_net = MatchingNet(img_shape, embedding_size=embedding_size, dropout_probality=dropout_probality, use_fce=use_fce)\n",
        "print(\"Model Summary\")\n",
        "print(matching_net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pr3IHrOWbZ3Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Move the model to GPU\n",
        "matching_net.to(device)\n",
        "# Create the optimizer\n",
        "optimizer = torch.optim.Adam(matching_net.parameters(), lr=lr)\n",
        "# placeholders to store our best performing metrics on Dev Set\n",
        "best_dev_loss = 0\n",
        "best_dev_accuracy = 0\n",
        "\n",
        "print(\"Beginning Training..\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  matching_net.train() # Set our model in train mode\n",
        "\n",
        "  # Placeholder variables to help track epoch loss and accuracy\n",
        "  total_loss = 0\n",
        "  total_accuracy = 0\n",
        "  \n",
        "  # Define our dataset and dataloader \n",
        "  train_dataset = OneShotDataset(augmented_train_dict, iterations)\n",
        "  train_dataset_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  num_workers=4,\n",
        "                                                  shuffle=False)\n",
        "  # Main training loop\n",
        "  for iteration, (support_images, support_labels, images, labels) in enumerate(train_dataset_loader):\n",
        "    # Move our data to GPU\n",
        "    support_batch_torch = support_images.to(device)\n",
        "    support_labels_torch = support_labels.to(device)\n",
        "    target_images_torch = images.to(device)\n",
        "    target_labels_torch =labels.to(device)\n",
        "    # Get the predictions and logits from our model\n",
        "    logits, predictions = matching_net(support_batch_torch, support_labels_torch, target_images_torch)\n",
        "    \n",
        "    # Calculate loss and accuracy in current iteration\n",
        "    loss = F.cross_entropy(logits, target_labels_torch)\n",
        "    accuracy = torch.mean((predictions == target_labels_torch).float())\n",
        "    \n",
        "    # Accumulate the values\n",
        "    total_loss = total_loss + loss.item()\n",
        "    total_accuracy = total_accuracy + accuracy.item()\n",
        "\n",
        "    # Perform Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  total_accuracy = total_accuracy/(iteration+1)\n",
        "  total_loss = total_loss/(iteration+1)\n",
        "  \n",
        "  # Log the training metrics to comet.ml\n",
        "  with pytorch_experiment.train():\n",
        "    pytorch_experiment.log_current_epoch(epoch)\n",
        "    pytorch_experiment.log_metric(\"loss\",total_loss)\n",
        "    pytorch_experiment.log_metric(\"accuracy\",total_accuracy)\n",
        "  print(f\"In epoch - {epoch} Train Set - Accuracy {total_accuracy} Loss - {total_loss} - for {(iteration+1)} iterations\")\n",
        "  \n",
        "  \n",
        "  # Run the model on Dev Set to evaluate performance on unseen data (every 3 epochs)\n",
        "  if epoch%3 == 0:\n",
        "    with pytorch_experiment.validate():\n",
        "      pytorch_experiment.log_current_epoch(epoch)\n",
        "      dev_accuracy, dev_loss = evaluate_model(matching_net, augmented_dev_dict)\n",
        "      print(f\"--In epoch - {epoch} Dev Set Accuracy - {dev_accuracy} Loss - {dev_loss}\")\n",
        "      \n",
        "      # Save the best performing model across all the epochs\n",
        "      if not best_dev_loss or dev_loss < best_dev_loss:\n",
        "        print(f\"---Found Better Model to save with Accuracy - {dev_accuracy} and loss - {dev_loss}\")\n",
        "        best_dev_loss = dev_loss\n",
        "        best_dev_accuracy = dev_accuracy\n",
        "        torch.save(matching_net.state_dict(), \"matching_net-%0.2f-accuracy.pt\"%(best_dev_accuracy))\n",
        "      \n",
        "      # Log the Dev metrics to comet.ml\n",
        "      pytorch_experiment.log_metric(\"accuracy\",dev_accuracy)\n",
        "      pytorch_experiment.log_metric(\"dev_loss\",dev_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GT5xu6IQH95t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Evaluating our model with the Test Data"
      ]
    },
    {
      "metadata": {
        "id": "EMVDyTxeOfMP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with pytorch_experiment.test():\n",
        "  matching_net_test = MatchingNet(img_shape, embedding_size=embedding_size, dropout_probality=dropout_probality, use_fce=use_fce)\n",
        "  matching_net_test.load_state_dict(torch.load(\"matching_net-%0.2f-accuracy.pt\"%(best_dev_accuracy)))\n",
        "  matching_net_test.to(device)\n",
        "  test_accuracy, test_loss = evaluate_model(matching_net_test, testing_dict, iterations=5000)\n",
        "  pytorch_experiment.log_metric(\"accuracy\",test_accuracy)\n",
        "  pytorch_experiment.log_metric(\"test_loss\",test_loss)\n",
        "  print(f\"Test Set Accuracy - {test_accuracy} Loss - {test_loss}\")\n",
        "pytorch_experiment.end()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-BtPdnZF39KX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pre-trained Models"
      ]
    },
    {
      "metadata": {
        "id": "eBYz9lru42BR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Downloading our weights"
      ]
    },
    {
      "metadata": {
        "id": "HONSuxvB2-kP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/RameshArvind/Pytorch-Matching-Networks.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OP1hG3lg3dF7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_weights_with_fce = \"Pytorch-Matching-Networks/matching_net-FCE-0.73-accuracy.pt\"\n",
        "model_weights_without_fce = \"Pytorch-Matching-Networks/matching_net-WITHOUT-FCE-0.74-accuracy.pt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZOTufWqm4BqM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Without FCE"
      ]
    },
    {
      "metadata": {
        "id": "1XrKQXGz3J1P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f80acd6a-116b-4ac1-a6bd-b6e8c0ee9f77"
      },
      "cell_type": "code",
      "source": [
        "embedding_size = 64\n",
        "dropout_probality=0.1\n",
        "use_fce = False\n",
        "\n",
        "matching_net_test = MatchingNet(img_shape, embedding_size=embedding_size, dropout_probality=dropout_probality, use_fce=use_fce)\n",
        "matching_net_test.load_state_dict(torch.load(model_weights_without_fce))\n",
        "matching_net_test.to(device)\n",
        "test_accuracy, test_loss = evaluate_model(matching_net_test, testing_dict, iterations=5000)\n",
        "pytorch_experiment.log_metric(\"accuracy\",test_accuracy)\n",
        "pytorch_experiment.log_metric(\"test_loss\",test_loss)\n",
        "print(f\"Without FCE - Test Set Accuracy - {test_accuracy} Loss - {test_loss}\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Without FCE - Test Set Accuracy - 0.7696000113487244 Loss - 2.3175869216918947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x9Cqt13Q4JPB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model With FCE"
      ]
    },
    {
      "metadata": {
        "id": "mJErbdR74Off",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c40dc3c-6258-4322-aa65-702f74ceba90"
      },
      "cell_type": "code",
      "source": [
        "embedding_size = 64\n",
        "dropout_probality=0.1\n",
        "use_fce = True\n",
        "\n",
        "matching_net_test = MatchingNet(img_shape, embedding_size=embedding_size, dropout_probality=dropout_probality, use_fce=use_fce)\n",
        "matching_net_test.load_state_dict(torch.load(model_weights_with_fce))\n",
        "matching_net_test.to(device)\n",
        "test_accuracy, test_loss = evaluate_model(matching_net_test, testing_dict, iterations=5000)\n",
        "pytorch_experiment.log_metric(\"accuracy\",test_accuracy)\n",
        "pytorch_experiment.log_metric(\"test_loss\",test_loss)\n",
        "print(f\"With FCE - Test Set Accuracy - {test_accuracy} Loss - {test_loss}\")"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With FCE - Test Set Accuracy - 0.7316000089645386 Loss - 2.3638008060455324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E8o_Xokt3_zn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Conclusion\n",
        "We are able to train a model that can solve the one shot 20-way classification problem to a reasonable degree with about **~73% with Fully Conditional Embedding** (and **~77% accuracy without it**) on the test set.\n",
        "\n",
        "Some of the approaches we can try to reach the reported 93.8% results are -\n",
        "\n",
        "\n",
        "1.   Use image augmentation\n",
        "2.   Automated Hyperparameter optimization\n",
        "3.   Utilize correct implmentation of the Fully conditional embedding for the target image.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "CZ-O4NLzfxY1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#TODO Items\n",
        "###<s>Make dataloader - To speed up training</s> - DONE - Loading all the numpy images at start speeds up things by a TON\n",
        "###<s>Figure out what optimizer was used - SGD/ADAM? </s>- ADAM WORKS\n",
        "### <s>Use image augmentation </s> - Done\n",
        "###<s>There is still something wrong with the network - Probably something with the way augmentation is done?</s> There isn't a problem with augmentation, rather the training and testing set provided by default have different \"distributions\". Need to investigate this further. Fixed by mixing the provided test and train set and creating new Train/Dev/Test sets.\n",
        "###<s>Setup a way to log training of models</s> - Done via integraion with comet.ml\n",
        "###Setup Hyperparameter Optimization\n",
        "###Use a high level pytorch API to setup training loops and Checkpointing - PyTorch Ignite\n",
        "###Using a dense layer at the end during embedding was causing the network to converge faser and better - Figure out why\n",
        "###<s>Work on the LSTM based embedding</s> - Done Albeit incorrectly\n",
        "### Implement Custom LSTM Cell which has hidden weights which can accept concatenated input\n"
      ]
    }
  ]
}
